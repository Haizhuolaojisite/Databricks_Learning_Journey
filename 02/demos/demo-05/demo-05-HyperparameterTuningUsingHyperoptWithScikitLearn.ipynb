{"cells":[{"cell_type":"markdown","source":["##### TODO Recording\n\nClick on Data in the left navigation pane select \"Browse DBFS\"\n\nGo to Data -> DBFS -> FileStore ->datasets-> Upload Bank Customer.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25b5d7a9-0ca5-4a93-a7e9-329d2eb1ac23","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n \nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport mlflow\nimport mlflow.sklearn\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"def33889-163f-4278-b519-4acecea2aba3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Loading the data\nDisplaying  the first 5 rows of the dataset\nlink=-https://www.kaggle.com/santoshd3/bank-customers?select=Churn+Modeling.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a239893-a4c3-4d48-b52a-d05ac38f9183","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["cust_attrition_data = pd.read_csv('/dbfs/FileStore/datasets/Bank_customer.csv')\n\ncust_attrition_data.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9455a224-0fea-473f-8ebd-234ef9307aa3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>101348.88</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>112542.58</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>113931.57</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>No</td>\n      <td>No</td>\n      <td>93826.63</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>79084.10</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>101348.88</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>112542.58</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>113931.57</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>No</td>\n      <td>No</td>\n      <td>93826.63</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>79084.10</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["cust_attrition_data .shape"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f33ec5e3-c9c2-45e2-9003-7d3e850a1cd2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: (10000, 13)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: (10000, 13)"]}}],"execution_count":0},{"cell_type":"code","source":["cust_attrition_data.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"381d93c7-e112-42e6-b0a4-86f6e099f7b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: Index(['CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',\n       'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n       'EstimatedSalary', 'Exited'],\n      dtype='object')","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: Index(['CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',\n       'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n       'EstimatedSalary', 'Exited'],\n      dtype='object')"]}}],"execution_count":0},{"cell_type":"markdown","source":["Checking unique values for each column.Customerid and Surname can be dropped"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08b4c5a4-b2a9-4a02-a26b-936d77ba4de3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["cust_attrition_data.nunique()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15562c47-5e5d-4d6b-9eac-bf72bc2e7102","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[6]: CustomerId         10000\nSurname             2932\nCreditScore          460\nGeography              3\nGender                 3\nAge                   70\nTenure                11\nBalance             6382\nNumOfProducts          4\nHasCrCard              2\nIsActiveMember         2\nEstimatedSalary     9999\nExited                 2\ndtype: int64","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[6]: CustomerId         10000\nSurname             2932\nCreditScore          460\nGeography              3\nGender                 3\nAge                   70\nTenure                11\nBalance             6382\nNumOfProducts          4\nHasCrCard              2\nIsActiveMember         2\nEstimatedSalary     9999\nExited                 2\ndtype: int64"]}}],"execution_count":0},{"cell_type":"markdown","source":["Features data and target data are made into separate dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"579fccaa-d5dd-4d3f-a34c-3eeb60c85aa4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["X = cust_attrition_data.drop(['CustomerId', 'Surname', 'Exited'], axis = 1)\ny = cust_attrition_data['Exited']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53f60694-3143-4d4f-92de-53df218917f7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Target variable is converted into numeric form"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4918f8ff-5d67-4a3a-818f-e5c78e402c2b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["class_dict = {'No': 0, 'Yes': 1}\n\ny = y.replace(class_dict)\ny"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07980fa6-e432-446d-8bc3-06fdcd5307cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: 0       1\n1       0\n2       1\n3       0\n4       0\n       ..\n9995    0\n9996    0\n9997    1\n9998    1\n9999    0\nName: Exited, Length: 10000, dtype: int64","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: 0       1\n1       0\n2       1\n3       0\n4       0\n       ..\n9995    0\n9996    0\n9997    1\n9998    1\n9999    0\nName: Exited, Length: 10000, dtype: int64"]}}],"execution_count":0},{"cell_type":"markdown","source":["Data is imbalanced"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4ffb442-be88-4e3e-95d9-a95b3b7aa98a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["y.value_counts()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"687c742e-bac9-4e03-9854-38390adcfca3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[9]: 0    7963\n1    2037\nName: Exited, dtype: int64","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[9]: 0    7963\n1    2037\nName: Exited, dtype: int64"]}}],"execution_count":0},{"cell_type":"markdown","source":["We are going to one hot encode categorical columns with dropping first category."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cf5a2b1-4849-4a6c-bbd5-32601d20b5f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["categoricalCols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\nnumericCols = ['Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\ncategorical_transformer = OneHotEncoder(drop = 'first')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69449ef0-3792-47f7-8b87-4f7b915472f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Preprocessing steps are defined with ColumnTransformer which will one hot encode only categorical colums and scale remainder numeric columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe3b2757-213c-416f-a1e7-6cbcb00a6016","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["preprocessor = ColumnTransformer(\n    transformers = [('cat', categorical_transformer, categoricalCols)], \n    remainder = StandardScaler() \n)\n\nprint(preprocessor)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bde357a-2e98-4dbb-a691-19150b559173","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ColumnTransformer(remainder=StandardScaler(),\n                  transformers=[('cat', OneHotEncoder(drop='first'),\n                                 ['Geography', 'Gender', 'HasCrCard',\n                                  'IsActiveMember'])])\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ColumnTransformer(remainder=StandardScaler(),\n                  transformers=[('cat', OneHotEncoder(drop='first'),\n                                 ['Geography', 'Gender', 'HasCrCard',\n                                  'IsActiveMember'])])\n"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)\n\nX_train.shape, X_test.shape"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67901c68-b332-443a-ba65-4f37fab76644","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: ((7000, 10), (3000, 10))","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: ((7000, 10), (3000, 10))"]}}],"execution_count":0},{"cell_type":"markdown","source":["Creating and train a decision tree classifier model and by using MLFLOW we are logging \n parameters, metrics, and the model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b16c164-0fef-4336-b5a4-f08699b87a69","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["with mlflow.start_run():\n \n    criterion= 'entropy'\n    max_depth = 5\n    max_features = 5\n    \n    dtc = DecisionTreeClassifier(criterion = criterion, max_depth = max_depth,\n                                 max_features = max_features)\n\n    pipeline = Pipeline( steps = [('preprocessor', preprocessor), ('classifier', dtc)])\n    pipeline.fit(X_train, y_train)\n\n    predictions =  pipeline.predict(X_test) \n    accuracy = accuracy_score(y_test, predictions)\n    precisionscore = precision_score(y_test, predictions)\n    recallscore = recall_score(y_test, predictions)\n    f1score = f1_score(y_test, predictions)\n  \n    mlflow.log_param('criterion', criterion)\n    mlflow.log_param('max_depth', max_depth)\n    mlflow.log_param('max_features', max_features)\n  \n    mlflow.log_metric('F1score',  f1score)\n    mlflow.log_metric('Recall_score',  recallscore)\n    mlflow.log_metric('Precision_score',  precisionscore)\n    mlflow.log_metric('Accuracy_score',  accuracy)\n\n    mlflow.sklearn.log_model(dtc, 'dtc_model')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1d275da-c222-485f-a3b1-bde128294951","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### TODO Recording:\n\n- Open up the Runs on the right hand side (using the flask icon)\n- Expand the metrics logged as well as the parameters we logged"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"012d00e2-3ffa-43ca-bde2-242d1e8f8e18","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Hyperopt is a Python library for hyperparameter tuning. Databricks Runtime for Machine Learning includes an optimized and enhanced version of Hyperopt, including automated MLflow tracking.\nThe objective is to find the best value for max features,max depth,and criterion\nMost of the code for a Hyperopt workflow is in the objective function.\nObjective function is defined\nWe are using  the f1 score( As the dataset is quite highly imbalanced) as our objective parameter to compare the models' performance which is to be maximised \nHyperopt tries to minimize the objective function. A F1 score means a better model, so you must return the negative f1 score.\n\nWhen calling fmin(), Databricks recommends active MLflow run management; that is, wrap the call to fmin() inside a with mlflow.start_run(): statement. This ensures that each fmin() call is logged to a separate MLflow main run, and makes it easier to log extra tags, parameters, or metrics to that run.\n\n#### TODO Recording:\n- Directly record with ML Flow enabled"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f7a4090-83f7-44a6-ae51-ac99fe39d94e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from hyperopt.pyll import scope\n\ndef objective(params):\n    \n    with mlflow.start_run(nested = True):\n\n        clf = DecisionTreeClassifier(max_features = params['max_features'], \n                                     max_depth = params['max_depth'],\n                                     criterion = params['criterion'])\n        pipeline = Pipeline(steps = [('preprocessor', preprocessor), ('model', clf)])\n        pipeline.fit(X_train, y_train)\n        \n        predictions =  pipeline.predict(X_test) \n        accuracy = accuracy_score(y_test, predictions)\n        precisionscore = precision_score(y_test, predictions)\n        recallscore = recall_score(y_test, predictions)\n        f1score = f1_score(y_test, predictions)\n\n        mlflow.log_metric('F1score',  f1score)\n        mlflow.log_metric('Recall_score',  recallscore)\n        mlflow.log_metric('Precision_score',  precisionscore)\n        mlflow.log_metric('Accuracy_score',  accuracy)\n\n        mlflow.sklearn.log_model(dtc, 'dtc_hpo')\n  \n        return {'loss': -f1score, 'status': STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bce1103e-9f90-461d-8d3b-5e4458b616f4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Search space for hyperparameters is defined"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"186286b5-bf72-40ff-8b60-b5b30d6a80ac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["search_space = {'max_features': scope.int(hp.quniform('max_features', 1, 10, 1)),\n                'max_depth': scope.int(hp.quniform('max_depth', 1, 15, 1)),\n                'criterion': hp.choice('criterion', ['gini', 'entropy'])} "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f49ab54f-c24a-4dae-a163-9f6e10dc3d38","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Algorithm is defined\n\nThe two main choices are:\n\nhyperopt.tpe.suggest: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on past results\nhyperopt.rand.suggest: Random search, a non-adaptive approach that samples over the search space"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53304429-c81a-491b-97e0-97a9fd08f3e1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["algo = tpe.suggest"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8a6905c-9f3a-4e22-9b97-491871b3fab3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now We are running  the tuning algorithm with Hyperopt fmin()\n\nSetting  max_evals to the maximum number of points in hyperparameter space to test, that is, the maximum number of models to fit and evaluate, in our case it is set as 16.\nBest value found out is with f1 score  around 57% with parameters '{'criterion': 1(entropy), 'max_depth':9.0, 'max_features':9.0}. max depth and max features are in float form as we are using hp.quniform and then for using those values in model we are casting those values into integer."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfba7cc4-df8d-427e-94d7-4ed196276136","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["argmin = fmin(\n  fn = objective,\n  space = search_space,\n  algo = algo,\n  max_evals = 16)\n\nprint('Best value found: ', argmin)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11015f83-309c-4152-8146-3d227e38ad44","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:03<00:54,  3.62s/trial, best loss: -0.5337781484570475]\r 12%|█▎        | 2/16 [00:06<00:48,  3.45s/trial, best loss: -0.5337781484570475]\r 19%|█▉        | 3/16 [00:10<00:44,  3.40s/trial, best loss: -0.5337781484570475]\r 25%|██▌       | 4/16 [00:13<00:39,  3.32s/trial, best loss: -0.5337781484570475]\r 31%|███▏      | 5/16 [00:16<00:36,  3.35s/trial, best loss: -0.5337781484570475]\r 38%|███▊      | 6/16 [00:20<00:33,  3.31s/trial, best loss: -0.5337781484570475]\r 44%|████▍     | 7/16 [00:23<00:30,  3.34s/trial, best loss: -0.5337781484570475]\r 50%|█████     | 8/16 [00:27<00:27,  3.41s/trial, best loss: -0.5337781484570475]\r 56%|█████▋    | 9/16 [00:30<00:24,  3.48s/trial, best loss: -0.5337781484570475]\r 62%|██████▎   | 10/16 [00:34<00:21,  3.55s/trial, best loss: -0.5337781484570475]\r 69%|██████▉   | 11/16 [00:38<00:17,  3.58s/trial, best loss: -0.5337781484570475]\r 75%|███████▌  | 12/16 [00:41<00:14,  3.58s/trial, best loss: -0.5337781484570475]\r 81%|████████▏ | 13/16 [00:45<00:10,  3.57s/trial, best loss: -0.5337781484570475]\r 88%|████████▊ | 14/16 [00:48<00:07,  3.55s/trial, best loss: -0.5337781484570475]\r 94%|█████████▍| 15/16 [00:52<00:03,  3.56s/trial, best loss: -0.5740181268882175]\r100%|██████████| 16/16 [00:55<00:00,  3.58s/trial, best loss: -0.576814326107446] \r100%|██████████| 16/16 [00:55<00:00,  3.49s/trial, best loss: -0.576814326107446]\nBest value found:  {'criterion': 1, 'max_depth': 9.0, 'max_features': 9.0}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:03<00:54,  3.62s/trial, best loss: -0.5337781484570475]\r 12%|█▎        | 2/16 [00:06<00:48,  3.45s/trial, best loss: -0.5337781484570475]\r 19%|█▉        | 3/16 [00:10<00:44,  3.40s/trial, best loss: -0.5337781484570475]\r 25%|██▌       | 4/16 [00:13<00:39,  3.32s/trial, best loss: -0.5337781484570475]\r 31%|███▏      | 5/16 [00:16<00:36,  3.35s/trial, best loss: -0.5337781484570475]\r 38%|███▊      | 6/16 [00:20<00:33,  3.31s/trial, best loss: -0.5337781484570475]\r 44%|████▍     | 7/16 [00:23<00:30,  3.34s/trial, best loss: -0.5337781484570475]\r 50%|█████     | 8/16 [00:27<00:27,  3.41s/trial, best loss: -0.5337781484570475]\r 56%|█████▋    | 9/16 [00:30<00:24,  3.48s/trial, best loss: -0.5337781484570475]\r 62%|██████▎   | 10/16 [00:34<00:21,  3.55s/trial, best loss: -0.5337781484570475]\r 69%|██████▉   | 11/16 [00:38<00:17,  3.58s/trial, best loss: -0.5337781484570475]\r 75%|███████▌  | 12/16 [00:41<00:14,  3.58s/trial, best loss: -0.5337781484570475]\r 81%|████████▏ | 13/16 [00:45<00:10,  3.57s/trial, best loss: -0.5337781484570475]\r 88%|████████▊ | 14/16 [00:48<00:07,  3.55s/trial, best loss: -0.5337781484570475]\r 94%|█████████▍| 15/16 [00:52<00:03,  3.56s/trial, best loss: -0.5740181268882175]\r100%|██████████| 16/16 [00:55<00:00,  3.58s/trial, best loss: -0.576814326107446] \r100%|██████████| 16/16 [00:55<00:00,  3.49s/trial, best loss: -0.576814326107446]\nBest value found:  {'criterion': 1, 'max_depth': 9.0, 'max_features': 9.0}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### TODO Recording\n\n- After running the code above, stay on this page and watch as more runs are added to the same experiment\n- Click on \"experiment\" and that will open up the Experiment on a new tab\n- There should be 17 runs in the Experiment\n- Sort by F1 score and find the run with the highest F1 score\n- Click on that and expand the \"Parameters\" and \"Metrics\" tab and show\n- IMPORTANT: Go to the experiments tab and delete all runs (so it's easier to see our next set of runs)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9668c87-3323-45d1-9308-72701f9211b1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["We are now using distributed tuning, adding one more argument to fmin(): a Trials class called SparkTrials.\n\nSparkTrials takes 2 optional arguments:\n\n- parallelism: Number of models to fit and evaluate concurrently. The default is the number of available Spark task slots.\n- timeout: Maximum time (in seconds) that fmin() can run. The default is no maximum time limit.\n\nThis example uses the same  simple objective function defined earlier. In this case, the function runs quickly and the overhead of starting the Spark jobs dominates the calculation time, so the calculations for the distributed case take more time. For typical real-world problems, the objective function is more complex, and using SparkTrails to distribute the calculations will be faster than single-machine tuning.\nAutomated MLflow tracking is enabled by default.\n\nBest value found out is with accuracy around 85% with parameters {'criterion': 0, 'max_depth': 8.033104023258073, 'max_features': 6.610009057572852}."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d884cd32-0a33-467d-b116-4e22e2c41460","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from hyperopt import SparkTrials\n \nspark_trials = SparkTrials()\n \nwith mlflow.start_run():\n    argmin = fmin(\n    fn = objective,\n    space = search_space,\n    algo = algo,\n    max_evals = 16,\n    trials = spark_trials)\n\nprint('Best value found: ', argmin)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"beedbfe9-81d1-407a-9fb2-4ed4c716834b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:08<02:02,  8.14s/trial, best loss: -0.39491916859122406]\r 12%|█▎        | 2/16 [00:09<00:55,  4.00s/trial, best loss: -0.5118644067796609] \r 19%|█▉        | 3/16 [00:10<00:34,  2.67s/trial, best loss: -0.5155642023346303]\r 25%|██▌       | 4/16 [00:12<00:28,  2.41s/trial, best loss: -0.5155642023346303]\r 31%|███▏      | 5/16 [00:15<00:28,  2.62s/trial, best loss: -0.5155642023346303]\r 38%|███▊      | 6/16 [00:16<00:21,  2.10s/trial, best loss: -0.5417406749555951]\r 44%|████▍     | 7/16 [00:18<00:18,  2.07s/trial, best loss: -0.5417406749555951]\r 50%|█████     | 8/16 [00:19<00:13,  1.73s/trial, best loss: -0.5417406749555951]\r 56%|█████▋    | 9/16 [00:23<00:17,  2.48s/trial, best loss: -0.5420393559928444]\r 62%|██████▎   | 10/16 [00:24<00:12,  2.05s/trial, best loss: -0.5456171735241503]\r 69%|██████▉   | 11/16 [00:26<00:10,  2.04s/trial, best loss: -0.5456171735241503]\r 75%|███████▌  | 12/16 [00:27<00:07,  1.76s/trial, best loss: -0.5495750708215298]\r 81%|████████▏ | 13/16 [00:31<00:07,  2.47s/trial, best loss: -0.5743494423791822]\r 88%|████████▊ | 14/16 [00:32<00:04,  2.03s/trial, best loss: -0.5743494423791822]\r100%|██████████| 16/16 [00:34<00:00,  1.55s/trial, best loss: -0.5743494423791822]\r100%|██████████| 16/16 [00:34<00:00,  2.18s/trial, best loss: -0.5743494423791822]\nTotal Trials: 16: 16 succeeded, 0 failed, 0 cancelled.\nBest value found:  {'criterion': 0, 'max_depth': 8.0, 'max_features': 9.0}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:08<02:02,  8.14s/trial, best loss: -0.39491916859122406]\r 12%|█▎        | 2/16 [00:09<00:55,  4.00s/trial, best loss: -0.5118644067796609] \r 19%|█▉        | 3/16 [00:10<00:34,  2.67s/trial, best loss: -0.5155642023346303]\r 25%|██▌       | 4/16 [00:12<00:28,  2.41s/trial, best loss: -0.5155642023346303]\r 31%|███▏      | 5/16 [00:15<00:28,  2.62s/trial, best loss: -0.5155642023346303]\r 38%|███▊      | 6/16 [00:16<00:21,  2.10s/trial, best loss: -0.5417406749555951]\r 44%|████▍     | 7/16 [00:18<00:18,  2.07s/trial, best loss: -0.5417406749555951]\r 50%|█████     | 8/16 [00:19<00:13,  1.73s/trial, best loss: -0.5417406749555951]\r 56%|█████▋    | 9/16 [00:23<00:17,  2.48s/trial, best loss: -0.5420393559928444]\r 62%|██████▎   | 10/16 [00:24<00:12,  2.05s/trial, best loss: -0.5456171735241503]\r 69%|██████▉   | 11/16 [00:26<00:10,  2.04s/trial, best loss: -0.5456171735241503]\r 75%|███████▌  | 12/16 [00:27<00:07,  1.76s/trial, best loss: -0.5495750708215298]\r 81%|████████▏ | 13/16 [00:31<00:07,  2.47s/trial, best loss: -0.5743494423791822]\r 88%|████████▊ | 14/16 [00:32<00:04,  2.03s/trial, best loss: -0.5743494423791822]\r100%|██████████| 16/16 [00:34<00:00,  1.55s/trial, best loss: -0.5743494423791822]\r100%|██████████| 16/16 [00:34<00:00,  2.18s/trial, best loss: -0.5743494423791822]\nTotal Trials: 16: 16 succeeded, 0 failed, 0 cancelled.\nBest value found:  {'criterion': 0, 'max_depth': 8.0, 'max_features': 9.0}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### TODO Recording\n\n- After running the code above, stay on this page and watch as more runs are added to the same experiment\n- Click on \"experiment\" and that will open up the Experiment on a new tab\n- There should be 17 runs in the Experiment\n- Sort by F1 score and find the run with the highest F1 score\n- Click on that and expand the \"Parameters\" and \"Metrics\" tab and show\n- To examine the effect of tuning a specific hyperparameter:\n- Select the resulting runs and click Compare.\n- In the Scatter Plot, select the hyperparameter from the X-axis drop-down menu and select metric from the Y-axis drop-down menu. (e.g. max_depth and F1 score)\n- IMPORTANT: Go to the experiments tab and delete all runs (so it's easier to see our next set of runs)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee64bfa7-8113-474f-bf7d-a28a493f337e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark_trials.results"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1324448d-ec66-4166-b918-53d83843c9a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[21]: [{'loss': -0.39491916859122406, 'status': 'ok'},\n {'loss': -0.5118644067796609, 'status': 'ok'},\n {'loss': -0.5155642023346303, 'status': 'ok'},\n {'loss': -0.4788441692466461, 'status': 'ok'},\n {'loss': -0.4968496849684968, 'status': 'ok'},\n {'loss': -0.5417406749555951, 'status': 'ok'},\n {'loss': -0.501039501039501, 'status': 'ok'},\n {'loss': -0.5014409221902018, 'status': 'ok'},\n {'loss': -0.5420393559928444, 'status': 'ok'},\n {'loss': -0.5456171735241503, 'status': 'ok'},\n {'loss': -0.4485981308411215, 'status': 'ok'},\n {'loss': -0.5495750708215298, 'status': 'ok'},\n {'loss': -0.5743494423791822, 'status': 'ok'},\n {'loss': -0.5014409221902018, 'status': 'ok'},\n {'loss': -0.5212669683257919, 'status': 'ok'},\n {'loss': -0.0, 'status': 'ok'}]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[21]: [{'loss': -0.39491916859122406, 'status': 'ok'},\n {'loss': -0.5118644067796609, 'status': 'ok'},\n {'loss': -0.5155642023346303, 'status': 'ok'},\n {'loss': -0.4788441692466461, 'status': 'ok'},\n {'loss': -0.4968496849684968, 'status': 'ok'},\n {'loss': -0.5417406749555951, 'status': 'ok'},\n {'loss': -0.501039501039501, 'status': 'ok'},\n {'loss': -0.5014409221902018, 'status': 'ok'},\n {'loss': -0.5420393559928444, 'status': 'ok'},\n {'loss': -0.5456171735241503, 'status': 'ok'},\n {'loss': -0.4485981308411215, 'status': 'ok'},\n {'loss': -0.5495750708215298, 'status': 'ok'},\n {'loss': -0.5743494423791822, 'status': 'ok'},\n {'loss': -0.5014409221902018, 'status': 'ok'},\n {'loss': -0.5212669683257919, 'status': 'ok'},\n {'loss': -0.0, 'status': 'ok'}]"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now, we examine four algorithms available in scikit-learn: support vector machines (SVM), random forest, and logistic regression and Decison tree\n\nIn the following cell, we are defining  a parameter params['type'] for the model name. This function also runs the training and calculates the cross-validation accuracy."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"961c632d-fef3-4c6b-a788-7cc4e5a90c2b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def objective(params):\n\n    with mlflow.start_run(nested = True):\n        classifier_type = params['type']\n        del params['type']\n        \n        if classifier_type == 'svm':\n            clf = SVC(**params)\n        elif classifier_type == 'rf':\n            clf = RandomForestClassifier(**params)\n        elif classifier_type == 'logreg':\n            clf = LogisticRegression(**params)\n        elif classifier_type == 'dtc':\n            clf = DecisionTreeClassifier(**params)\n        else:\n            return 0\n        \n        pipeline = Pipeline(steps = [('preprocessor', preprocessor), ('model', clf)])\n        pipeline.fit(X_train, y_train)\n        \n        predictions =  pipeline.predict(X_test) \n        accuracy = accuracy_score(y_test, predictions)\n        precisionscore = precision_score(y_test, predictions)\n        recallscore = recall_score(y_test, predictions)\n        f1score = f1_score(y_test, predictions)\n        \n        mlflow.log_metric('F1score',  f1score)\n        mlflow.log_metric('Recall_score',  recallscore)\n        mlflow.log_metric('Precision_score',  precisionscore)\n        mlflow.log_metric('Accuracy_score',  accuracy)\n        \n        mlflow.sklearn.log_model(clf, 'clf_hpo')\n\n        return {'loss': -f1score, 'status': STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc859cf3-5aa9-44de-94c7-de551f06a48a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Search space is defined for multiple models\nUsing hp.choice to select different models."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf70fd01-5059-4d6b-8d28-4e8a18e25a7a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["search_space = hp.choice('classifier_type', [\n    {\n        'type': 'svm',\n        'C': hp.lognormal('SVM_C', 0, 1.0),\n        'kernel': hp.choice('kernel', ['linear', 'rbf'])\n    },\n    {\n        'type': 'rf',\n        'n_estimators':scope.int(hp.quniform('n_estimators', 100, 500, 50)),\n        'max_depth': scope.int(hp.quniform('max_depth_rf', 2, 20 , 1)),\n        'criterion': hp.choice('criterion_rf', ['gini', 'entropy'])\n    },\n    {\n        'type': 'logreg',\n        'C': hp.lognormal('LR_C', 0, 1.0),\n        'solver': hp.choice('solver', ['liblinear', 'lbfgs'])\n    },\n  \n    {\n        'type': 'dtc',\n        'max_features':scope.int(hp.quniform('max_features', 1,10,1)),\n        'max_depth': scope.int(hp.quniform('max_depth_dtc', 2, 20, 1)),\n        'criterion': hp.choice('criterion_dtc', ['gini', 'entropy'])\n    }\n    \n    \n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71b4ce53-37c5-468e-98ef-b0e7ba299cce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Same steps are repeated as done for single model case. This time Best accuracy obtained is around 86% with RF model with hyperparameters {'criterion': 'gini', 'max_depth': 11, 'n_estimators': 100, 'type': 'rf'}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b767363d-c818-4413-b49f-454373cb6ccf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["algo = tpe.suggest\n\nspark_trials = SparkTrials()\n\nwith mlflow.start_run():\n    best_result = fmin(\n        fn = objective, \n        space = search_space,\n        algo = algo,\n        max_evals = 32,\n        trials = spark_trials)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54224611-457e-440a-aa38-0d585ee512e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/32 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 2/32 [00:09<02:17,  4.59s/trial, best loss: -0.5202821869488536]\r  9%|▉         | 3/32 [00:11<01:43,  3.58s/trial, best loss: -0.545068928950159] \r 12%|█▎        | 4/32 [00:12<01:13,  2.62s/trial, best loss: -0.545068928950159]\r 16%|█▌        | 5/32 [00:16<01:23,  3.10s/trial, best loss: -0.545068928950159]\r 22%|██▏       | 7/32 [00:18<00:51,  2.06s/trial, best loss: -0.545068928950159]\r 25%|██▌       | 8/32 [00:19<00:43,  1.81s/trial, best loss: -0.5504273504273504]\r 28%|██▊       | 9/32 [00:24<01:01,  2.68s/trial, best loss: -0.5504273504273504]\r 31%|███▏      | 10/32 [00:25<00:48,  2.21s/trial, best loss: -0.5504273504273504]\r 34%|███▍      | 11/32 [00:27<00:45,  2.16s/trial, best loss: -0.5504273504273504]\r 38%|███▊      | 12/32 [00:31<00:54,  2.74s/trial, best loss: -0.5566343042071198]\r 41%|████      | 13/32 [00:35<00:59,  3.11s/trial, best loss: -0.5566343042071198]\r 44%|████▍     | 14/32 [00:36<00:45,  2.52s/trial, best loss: -0.5850202429149798]\r 47%|████▋     | 15/32 [00:37<00:35,  2.07s/trial, best loss: -0.5850202429149798]\r 50%|█████     | 16/32 [00:38<00:28,  1.75s/trial, best loss: -0.5850202429149798]\r 53%|█████▎    | 17/32 [00:42<00:36,  2.44s/trial, best loss: -0.5850202429149798]\r 56%|█████▋    | 18/32 [00:44<00:32,  2.31s/trial, best loss: -0.5850202429149798]\r 62%|██████▎   | 20/32 [00:46<00:20,  1.71s/trial, best loss: -0.5850202429149798]\r 66%|██████▌   | 21/32 [00:53<00:33,  3.03s/trial, best loss: -0.5850202429149798]\r 69%|██████▉   | 22/32 [00:55<00:25,  2.52s/trial, best loss: -0.5853658536585366]\r 72%|███████▏  | 23/32 [00:59<00:26,  2.94s/trial, best loss: -0.5853658536585366]\r 75%|███████▌  | 24/32 [01:01<00:21,  2.69s/trial, best loss: -0.5853658536585366]\r 78%|███████▊  | 25/32 [01:05<00:21,  3.07s/trial, best loss: -0.5853658536585366]\r 81%|████████▏ | 26/32 [01:06<00:14,  2.47s/trial, best loss: -0.5853658536585366]\r 84%|████████▍ | 27/32 [01:10<00:14,  2.94s/trial, best loss: -0.5853658536585366]\r 88%|████████▊ | 28/32 [01:13<00:11,  2.96s/trial, best loss: -0.5853658536585366]\r 91%|█████████ | 29/32 [01:15<00:08,  2.68s/trial, best loss: -0.5853658536585366]\r 94%|█████████▍| 30/32 [01:17<00:04,  2.48s/trial, best loss: -0.5853658536585366]\r 97%|█████████▋| 31/32 [01:19<00:02,  2.34s/trial, best loss: -0.5853658536585366]\r100%|██████████| 32/32 [01:21<00:00,  2.24s/trial, best loss: -0.5853658536585366]\r100%|██████████| 32/32 [01:21<00:00,  2.54s/trial, best loss: -0.5853658536585366]\nTotal Trials: 32: 32 succeeded, 0 failed, 0 cancelled.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/32 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 2/32 [00:09<02:17,  4.59s/trial, best loss: -0.5202821869488536]\r  9%|▉         | 3/32 [00:11<01:43,  3.58s/trial, best loss: -0.545068928950159] \r 12%|█▎        | 4/32 [00:12<01:13,  2.62s/trial, best loss: -0.545068928950159]\r 16%|█▌        | 5/32 [00:16<01:23,  3.10s/trial, best loss: -0.545068928950159]\r 22%|██▏       | 7/32 [00:18<00:51,  2.06s/trial, best loss: -0.545068928950159]\r 25%|██▌       | 8/32 [00:19<00:43,  1.81s/trial, best loss: -0.5504273504273504]\r 28%|██▊       | 9/32 [00:24<01:01,  2.68s/trial, best loss: -0.5504273504273504]\r 31%|███▏      | 10/32 [00:25<00:48,  2.21s/trial, best loss: -0.5504273504273504]\r 34%|███▍      | 11/32 [00:27<00:45,  2.16s/trial, best loss: -0.5504273504273504]\r 38%|███▊      | 12/32 [00:31<00:54,  2.74s/trial, best loss: -0.5566343042071198]\r 41%|████      | 13/32 [00:35<00:59,  3.11s/trial, best loss: -0.5566343042071198]\r 44%|████▍     | 14/32 [00:36<00:45,  2.52s/trial, best loss: -0.5850202429149798]\r 47%|████▋     | 15/32 [00:37<00:35,  2.07s/trial, best loss: -0.5850202429149798]\r 50%|█████     | 16/32 [00:38<00:28,  1.75s/trial, best loss: -0.5850202429149798]\r 53%|█████▎    | 17/32 [00:42<00:36,  2.44s/trial, best loss: -0.5850202429149798]\r 56%|█████▋    | 18/32 [00:44<00:32,  2.31s/trial, best loss: -0.5850202429149798]\r 62%|██████▎   | 20/32 [00:46<00:20,  1.71s/trial, best loss: -0.5850202429149798]\r 66%|██████▌   | 21/32 [00:53<00:33,  3.03s/trial, best loss: -0.5850202429149798]\r 69%|██████▉   | 22/32 [00:55<00:25,  2.52s/trial, best loss: -0.5853658536585366]\r 72%|███████▏  | 23/32 [00:59<00:26,  2.94s/trial, best loss: -0.5853658536585366]\r 75%|███████▌  | 24/32 [01:01<00:21,  2.69s/trial, best loss: -0.5853658536585366]\r 78%|███████▊  | 25/32 [01:05<00:21,  3.07s/trial, best loss: -0.5853658536585366]\r 81%|████████▏ | 26/32 [01:06<00:14,  2.47s/trial, best loss: -0.5853658536585366]\r 84%|████████▍ | 27/32 [01:10<00:14,  2.94s/trial, best loss: -0.5853658536585366]\r 88%|████████▊ | 28/32 [01:13<00:11,  2.96s/trial, best loss: -0.5853658536585366]\r 91%|█████████ | 29/32 [01:15<00:08,  2.68s/trial, best loss: -0.5853658536585366]\r 94%|█████████▍| 30/32 [01:17<00:04,  2.48s/trial, best loss: -0.5853658536585366]\r 97%|█████████▋| 31/32 [01:19<00:02,  2.34s/trial, best loss: -0.5853658536585366]\r100%|██████████| 32/32 [01:21<00:00,  2.24s/trial, best loss: -0.5853658536585366]\r100%|██████████| 32/32 [01:21<00:00,  2.54s/trial, best loss: -0.5853658536585366]\nTotal Trials: 32: 32 succeeded, 0 failed, 0 cancelled.\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### TODO Recording\n\n- After running the code above, stay on this page and watch as more runs are added to the same experiment\n- Click on \"experiment\" and that will open up the Experiment on a new tab\n- There should be 33 runs in the Experiment\n- Sort by F1 score and find the run with the highest F1 score\n- Click on that and expand the \"Parameters\" and \"Metrics\" tab and show"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d02b74b6-486a-4db4-bfdb-c4247fbeebd1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Using hyperopt.space_eval to display the results of the hyperparameter search."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3eee9ce7-6b92-40fd-af19-e2d7a5021bfa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import hyperopt\n\nprint(hyperopt.space_eval(search_space, best_result))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3da93fb5-56ac-4641-a6e3-63ca52527bd8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"{'criterion': 'gini', 'max_depth': 20, 'n_estimators': 300, 'type': 'rf'}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["{'criterion': 'gini', 'max_depth': 20, 'n_estimators': 300, 'type': 'rf'}\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"demo-05-HyperparameterTuningUsingHyperoptWithScikitLearn","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":733115242254438}},"nbformat":4,"nbformat_minor":0}
